# üìù Cahier des Charges : Syst√®me RAG & G√©n√©ration Documentaire

## 1. Objectifs du Projet
Mettre en place un syst√®me conversationnel intelligent capable de :
1.  **Valoriser l'exp√©rience** : R√©pondre aux questions des recruteurs en se basant strictement sur les exp√©riences r√©elles de Philippe Gehringer (RAG).
2.  **Analyser le besoin** : Comprendre une fiche de poste (JD - Job Description) upload√©e par un utilisateur.
3.  **Convaincre** : G√©n√©rer automatiquement des documents PDF ultra-personnalis√©s (CV cibl√© ou Lettre de motivation/Offre de services) via Gotenberg.

---

## 2. Parcours Utilisateur (User Stories)

### Sc√©nario A : La Conversation "Curieuse" (Mode Q&A)
*   **Acteur** : Un visiteur (recruteur potentiel).
*   **Action** : Pose une question (ex: *"Avez-vous de l'exp√©rience dans l'industrie automobile et la RSE ?"*).
*   **Syst√®me** :
    1.  Interroge Qdrant (`experience_pro`) pour trouver les segments de CV pertinents.
    2.  Le LLM synth√©tise une r√©ponse : *"Oui, j'ai men√© telle mission chez X o√π j'ai r√©duit l'empreinte carbone de Y%..."*.
    3.  L'IA sugg√®re des questions de suivi.

### Sc√©nario B : L'Attaque "Offre de Service" (Mode Analyse de Doc)
*   **Acteur** : Un visiteur upload un fichier PDF/Docx (une fiche de poste).
*   **Action** : Le visiteur ne dit rien ou dit "Est-ce que ce poste vous correspond ?".
*   **Syst√®me** :
    1.  Analyse le document pour extraire les comp√©tences cl√©s et les enjeux.
    2.  Cherche dans Qdrant les exp√©riences de Philippe qui "match" ces points sp√©cifiques.
    3.  L'IA r√©pond : *"Ce poste correspond √† 85% √† mon profil. Mes exp√©riences chez A et B sont directement transposables. Voulez-vous que je g√©n√®re une lettre de motivation cibl√©e pour ce poste ?"*.
*   **Action** : Le visiteur clique "Oui".
*   **Syst√®me** : G√©n√®re un PDF via Gotenberg et fournit le lien de t√©l√©chargement dans le chat.

---

## 3. Architecture Technique & Flux de Donn√©es

### 3.1. Stack Technique
*   **Frontend** : ChatWidget React (existant).
*   **Orchestrateur** : n8n.
*   **M√©moire (Vector DB)** : Qdrant (Collection `experience_pro`).
*   **Intelligence (LLM)** : Ollama (Mod√®le sugg√©r√© : `llama3` ou `mistral` pour le chat, `nomic-embed-text` pour les vecteurs).
*   **Rendu Document** : Gotenberg (API Docker).

### 3.2. Mod√©lisation des Donn√©es (Qdrant)
La collection `experience_pro` ne doit pas contenir des blocs de texte bruts, mais des objets structur√©s pour un RAG efficace.

**Structure d'un vecteur (Payload) :**
*La qualit√© du RAG d√©pend de la structuration. Plut√¥t que de "lire" un CV PDF brut, nous allons utiliser un format structur√© (CSV) qui sera bien plus puissant pour la recherche.*

```json
{
  "id": "uuid",
  "client_sector": "Automobile",
  "role": "Manager de transition",
  "context": "Fermeture de site...",
  "actions": "N√©gociation syndicale, plan social...",
  "results": "0 jour de gr√®ve, budget respect√©...",
  "skills": ["RSE", "Gestion de crise", "Leadership"],
  "tools": ["Excel", "SAP"],
  "start_date": "2020-01",
  "end_date": "2020-12"
}
```

---

## 4. Sp√©cifications Fonctionnelles n8n

Nous devons cr√©er/affiner 3 workflows distincts.

### W1. Ingestion (Alimentation de la base)
*Ce workflow est d√©clench√© manuellement par Philippe pour mettre √† jour son "Double Num√©rique".*
1.  **Input** : Fichier JSON ou CSV contenant toutes les exp√©riences pass√©es + Comp√©tences + Certifications.
2.  **Split** : D√©coupage par mission.
3.  **Embedding** : Ollama transforme le texte en vecteurs.
4.  **Upsert** : Envoi dans Qdrant (`experience_pro`).

### W2. Le Cerveau Conversationnel (RAG)
*D√©clench√© par un message texte.*
1.  **Embedding Query** : La question utilisateur est vectoris√©e.
2.  **Search** : Qdrant cherche les 3 √† 5 "chunks" les plus proches.
3.  **System Prompt Construction** :
    > "Tu es l'assistant IA de Philippe Gehringer. Voici des extraits de son exp√©rience : [CONTEXTE QDRANT]. R√©ponds √† la question [QUESTION USER] en utilisant UNIQUEMENT ce contexte. Adopte un ton professionnel, expert mais accessible."
4.  **Generation** : Ollama g√©n√®re la r√©ponse.
5.  **Output** : Renvoi vers le ChatWidget.

### W3. Le G√©n√©rateur de Documents (Analyst + Builder)
*D√©clench√© par un upload de fichier.*
1.  **Parse** : n8n lit le fichier (PDF/Word).
2.  **Extract** : LLM extrait : `Nom Entreprise`, `Besoin Principal`, `3 Comp√©tences Cl√©s requises`.
3.  **Matching** :
    *   Recherche Qdrant pour la Comp√©tence A.
    *   Recherche Qdrant pour la Comp√©tence B.
    *   Recherche Qdrant pour la Comp√©tence C.
4.  **Drafting** : Le LLM r√©dige le corps de la lettre en connectant `Besoin` <-> `Exp√©rience trouv√©e`.
5.  **Templating** : Injection du texte dans un template HTML (CSS Tailwind pour le style).
6.  **Rendering** : Envoi du HTML √† Gotenberg -> Retourne un PDF.
7.  **Delivery** : Stockage temporaire + Envoi du lien au Chat.

---

## 5. Livrables Attendus & T√¢ches

### Phase 1 : Consolidation des Donn√©es (La base)
- [ ] D√©finir le sch√©ma JSON exact des exp√©riences.
- [ ] Cr√©er le Workflow W1 (Ingestion) et charger l'historique de Philippe.

### Phase 2 : Le Chat RAG (Le dialogue)
- [ ] Configurer Qdrant pour la recherche s√©mantique.
- [ ] Optimiser le "System Prompt" pour √©viter que l'IA ne parle √† la 3√®me personne (elle doit dire "Philippe a fait..." ou "J'ai fait..." selon votre choix).

### Phase 3 : L'Analyse & G√©n√©ration (La "Magic Feature")
- [ ] Cr√©er le Template HTML pour le CV/Lettre (Clean, Design Gehringer).
- [ ] Connecter Gotenberg dans n8n.
- [ ] Tester l'analyse de Fiche de Poste.

## 6. D√©cisions Techniques Valid√©es (au 11/12/2025)

1.  **Identit√© IA** :
    *   **Persona** : "Je suis l'IA de Philippe".
    *   **Ton** : L'IA assume sa nature artificielle mais parle avec l'autorit√© d√©l√©gu√©e par Philippe. Elle utilise le "Nous" quand elle parle du bin√¥me Philippe+IA, et "Philippe" quand elle parle de l'humain.

2.  **Source de Donn√©es (Ingestion)** :
    *   **Approche Hybride** :
        *   **Source Primaire (Haute Qualit√©)** : Un fichier structur√© (CSV/Excel) d√©taill√© ("Table d'Exp√©riences") que Philippe remplira. C'est la source de v√©rit√© pour le RAG.
        *   **Source Secondaire** : Les PDF existants (CVs) serviront d'appoint ou de pi√®ces jointes, mais le RAG tapera dans la donn√©e structur√©e pour √©viter les hallucinations de mise en page.

3.  **Stockage & R√©tention Documents** :
    *   **Archivage** : Les PDF g√©n√©r√©s (CVs cibl√©s, Lettres) sont stock√©s localement sur le serveur.
    *   **Feedback Loop** : Une trace de chaque g√©n√©ration est gard√©e (dans Qdrant ou une table SQL) pour que l'IA puisse dire "J'ai d√©j√† g√©n√©r√© un CV pour ce type de poste la semaine derni√®re".

4.  **Format de Sortie** :
    *   Stockage : Volume Docker partag√© ou dossier local `generated-docs/`.
    *   Acc√®s : Lien de t√©l√©chargement unique fourni dans le Chat.

## 7. Architecture Impl√©ment√©e (Mise √† jour 12/12/2025)

Cette section documente les sp√©cificit√©s techniques critiques mises en place lors de l'impl√©mentation finale des workflows.

### 7.1. Workflow Unifi√© "RAG Chat & PDF Generator"

Ce workflow combine la conversation, l'analyse de fichiers et la g√©n√©ration de documents.

*   **D√©tection d'Entr√©e (Robustesse Input)** :
    *   Les entr√©es fichiers peuvent arriver sous deux formes selon la source (Chat Widget vs Formulaire Web/MCP) : soit dans l'objet `binary` standard, soit dans `json.files`.
    *   **Solution** : Un n≈ìud **"Check Input"** normalise cette entr√©e.
    *   *Code Cl√©* :
        ```javascript
        const hasBinary = items[0].binary && Object.keys(items[0].binary).length > 0;
        const hasJsonFile = items[0].json.files && items[0].json.files.length > 0;
        // ... Logique de fusion
        ```

*   **Gestion des Fichiers PDF (Analyse)** :
    *   L'Agent LangChain ne lit pas nativement les binaires n8n.
    *   **Strat√©gie** : Extraction du texte en amont (via `pdf-parse` ou extraction native) et injection dans le prompt utilisateur (`chatInput`).
    *   L'IA re√ßoit donc : "Voici le contenu du fichier joint : [TEXTE DU PDF]... Analyse ceci."

*   **G√©n√©ration PDF "Invisible"** :
    *   L'Agent a pour instruction de g√©n√©rer du code HTML entour√© de balises `<GENERATE_PDF>...</GENERATE_PDF>` s'il d√©cide de cr√©er un document.
    *   Un routeur JavaScript (`Detect PDF Request`) scanne la r√©ponse.
    *   Si balise trouv√©e : Extraction du HTML -> Envoi √† Gotenberg -> Sauvegarde du PDF.
    *   Si pas de balise : R√©ponse textuelle directe √† l'utilisateur.

### 7.2. Workflow "Backend Chat API" (Int√©gration Astro)

Ce workflow g√®re les requ√™tes API provenants du site web (Chat Widget React).

*   **Cha√Æne Audio (Whisper STT)** :
    *   **Probl√®me** : Les donn√©es binaires audio (`audio/webm`) √©taient perdues lors de la travers√©e des n≈ìuds de logique (JWT Auth, Router).
    *   **Solution** : Injection chirurgicale juste avant le n≈ìud Whisper.
        ```javascript
        // R√©cup√©ration directe depuis la source
        const binaryData = $('Webhook (Entr√©e Unique)').first().binary;
        return [{ json: items[0].json, binary: binaryData }];
        ```
    *   **Configuration Whisper** : Champ d'entr√©e binaire d√©fini sur `audio` (au lieu de `data` par d√©faut) pour correspondre au FormData du frontend.

*   **Parsing de la R√©ponse IA** :
    *   Le n≈ìud **OpenAI LangChain** retourne une structure JSON complexe et imbriqu√©e (`output.content[0].text`) qui n'est pas toujours expos√©e clairement dans l'interface n8n ("Run Once for All Items" vs "Each Item").
    *   **Solution** : Un n≈ìud **Code** de nettoyage final assure une sortie stable vers le Frontend.
        ```javascript
        /* Extraction robuste de la r√©ponse texte */
        const content = item.json.output[0].content;
        const text = content ? content[0].text : "Pas de r√©ponse";
        return { json: { text } };
        ```
    *   Le n≈ìud "Respond to Webhook" renvoie alors un JSON simple : `{ "response": "{{ $json.text }}" }`.
